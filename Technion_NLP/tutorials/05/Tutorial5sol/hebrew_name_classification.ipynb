{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "## Class Assignment - Classifying first name -> Gender"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# In order to use the solution as shown in the class (for classifying last names to nationality)\n",
    "# I have scraped 3 hebrew sites for names:\n",
    "# 1. https://babynames.baby-land.co.il\n",
    "# 2. https://www.israelibaby.co.il\n",
    "# 3. https://www.baby-names.co.il\n",
    "\n",
    "# After generating two lists of boys and girls name, I have created 3 files, one for each gender\n",
    "# boy, girl and unisex\n",
    "# in the same structure as was used in last name classification\n",
    "# Also, the letters are now hebrew and not latin\n",
    "\n",
    "# the rest of the notebook stayed the same\n",
    "\n",
    "# The revised cells are marked with ** Eyal **"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rykvZaURQhFN",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "NLP From Scratch: Classifying Names with a Character-Level RNN\n",
    "**************************************************************\n",
    "**Author**: `Sean Robertson <https://github.com/spro/practical-pytorch>`_\n",
    "\n",
    "We will be building and training a basic character-level RNN to classify\n",
    "words. This tutorial, along with the following two, show how to do\n",
    "preprocess data for NLP modeling \"from scratch\", in particular not using\n",
    "many of the convenience functions of `torchtext`, so you can see how\n",
    "preprocessing for NLP modeling works at a low level.\n",
    "\n",
    "A character-level RNN reads words as a series of characters -\n",
    "outputting a prediction and \"hidden state\" at each step, feeding its\n",
    "previous hidden state into each next step. We take the final prediction\n",
    "to be the output, i.e. which class the word belongs to.\n",
    "\n",
    "Specifically, we'll train on a few thousand surnames from 18 languages\n",
    "of origin, and predict which language a name is from based on the\n",
    "spelling:\n",
    "\n",
    "::\n",
    "\n",
    "    $ python predict.py Hinton\n",
    "    (-0.47) Scottish\n",
    "    (-1.52) English\n",
    "    (-3.57) Irish\n",
    "\n",
    "    $ python predict.py Schmidhuber\n",
    "    (-0.19) German\n",
    "    (-2.48) Czech\n",
    "    (-2.68) Dutch\n",
    "\n",
    "\n",
    "**Recommended Reading:**\n",
    "\n",
    "I assume you have at least installed PyTorch, know Python, and\n",
    "understand Tensors:\n",
    "\n",
    "-  https://pytorch.org/ For installation instructions\n",
    "-  :doc:`/beginner/deep_learning_60min_blitz` to get started with PyTorch in general\n",
    "-  :doc:`/beginner/pytorch_with_examples` for a wide and deep overview\n",
    "-  :doc:`/beginner/former_torchies_tutorial` if you are former Lua Torch user\n",
    "\n",
    "It would also be useful to know about RNNs and how they work:\n",
    "\n",
    "-  `The Unreasonable Effectiveness of Recurrent Neural\n",
    "   Networks <https://karpathy.github.io/2015/05/21/rnn-effectiveness/>`__\n",
    "   shows a bunch of real life examples\n",
    "-  `Understanding LSTM\n",
    "   Networks <https://colah.github.io/posts/2015-08-Understanding-LSTMs/>`__\n",
    "   is about LSTMs specifically but also informative about RNNs in\n",
    "   general\n",
    "\n",
    "Preparing the Data\n",
    "==================\n",
    "\n",
    ".. Note::\n",
    "   Download the data from\n",
    "   `here <https://download.pytorch.org/tutorial/data.zip>`_\n",
    "   and extract it to the current directory.\n",
    "\n",
    "Included in the ``data/names`` directory are 18 text files named as\n",
    "\"[Language].txt\". Each file contains a bunch of names, one name per\n",
    "line, mostly romanized (but we still need to convert from Unicode to\n",
    "ASCII).\n",
    "\n",
    "We'll end up with a dictionary of lists of names per language,\n",
    "``{language: [names ...]}``. The generic variables \"category\" and \"line\"\n",
    "(for language and name in our case) are used for later extensibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# ** Eyal **\n",
    "\n",
    "hebrew_letters = \"אבגדהוזחטיכלמנסעפצקרשת\"\n",
    "hebrew_suffix_letters = \"ךםןףץ\"\n",
    "\n",
    "all_letters = hebrew_letters + hebrew_suffix_letters + \" .,;'\"\n",
    "\n",
    "n_letters = len(all_letters)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# ** Eyal **\n",
    "\n",
    "#  Imports\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping babynames ...\n",
      "Boys ...\n",
      "Girls ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 183/183 [00:00<00:00, 349525.33it/s]\n",
      "100%|██████████| 299/299 [00:00<00:00, 407836.39it/s]\n"
     ]
    }
   ],
   "source": [
    "# ** Eyal **\n",
    "\n",
    "#  Webscarping https://babynames.baby-land.co.il\n",
    "\n",
    "def create_from_babynames():\n",
    "\n",
    "   print(\"Scraping babynames ...\")\n",
    "   base_url = \"https://babynames.baby-land.co.il/namelist/?_sft_name_gender={}\"\n",
    "\n",
    "   boys_names = []\n",
    "   girls_names = []\n",
    "   unisex_names = []\n",
    "\n",
    "   print(\"Boys ...\")\n",
    "   req = requests.get(base_url.format(\"boy\"))\n",
    "   soup = bs(req.text, 'html.parser')\n",
    "   for t in soup.select('a[class=boys]'):\n",
    "       boys_names.append(t.text)\n",
    "\n",
    "   print(\"Girls ...\")\n",
    "   req = requests.get(base_url.format(\"girl\"))\n",
    "   soup = bs(req.text, 'html.parser')\n",
    "   for t in tqdm.tqdm(soup.select('a[class=girl]')):\n",
    "       girls_names.append(t.text)\n",
    "\n",
    "   req = requests.get(base_url.format(\"unisex\"))\n",
    "   soup = bs(req.text, 'html.parser')\n",
    "   for t in tqdm.tqdm(soup.select('a[class=unisex]')):\n",
    "       unisex_names.append(t.text)\n",
    "\n",
    "   return boys_names + unisex_names, girls_names + unisex_names\n",
    "\n",
    "boys_names1, girls_names1 = create_from_babynames()\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping isralibaby ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:29<00:00,  1.36s/it]\n"
     ]
    }
   ],
   "source": [
    "# ** Eyal **\n",
    "\n",
    "import tqdm\n",
    "\n",
    "#  Webscarping https://www.israelibaby.co.il\n",
    "\n",
    "def create_from_israelibaby():\n",
    "    \"\"\"\n",
    "\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    print(\"Scraping isralibaby ...\")\n",
    "    base_url = \"https://www.israelibaby.co.il/name-{}-{}\"\n",
    "\n",
    "    boys_names = []\n",
    "    girls_names = []\n",
    "\n",
    "    for letter in tqdm.tqdm(range(len(hebrew_letters))):\n",
    "        # boys\n",
    "        url = base_url.format(1, letter)\n",
    "        req = requests.get(url)\n",
    "        soup = bs(req.text, 'html.parser')\n",
    "        for t in soup.select('div[class=name]'):\n",
    "            boys_names.append(t.text)\n",
    "\n",
    "        # girls\n",
    "        url = base_url.format(2, letter)\n",
    "        req = requests.get(url)\n",
    "        soup = bs(req.text, 'html.parser')\n",
    "        for t in soup.select('div[class=name]'):\n",
    "            girls_names.append(t.text)\n",
    "\n",
    "    return boys_names, girls_names\n",
    "\n",
    "boys_names2, girls_names2 = create_from_israelibaby()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping baby_names ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [01:13<00:00,  3.35s/it]\n"
     ]
    }
   ],
   "source": [
    "# ** Eyal **\n",
    "\n",
    "#  Webscarping https://www.baby-names.co.il\n",
    "\n",
    "def create_from_baby_names():\n",
    "    \"\"\"\n",
    "    Scrapes the 'https://www.baby-names.co.il' site,\n",
    "    Goes over the boys site, and for each letter\n",
    "    1. Check for pages\n",
    "    2. Get the names from the page(s)\n",
    "    Do the same for the girls\n",
    "\n",
    "    :return: the boys list and the girl list\n",
    "    \"\"\"\n",
    "    print(\"Scraping baby_names ...\")\n",
    "\n",
    "    boys_base_url = 'https://www.baby-names.co.il/category/%D7%9B%D7%9C-%D7%94%D7%A9%D7%9E%D7%95%D7%AA/%D7%A9%D7%9E%D7%95%D7%AA-%D7%9C%D7%91%D7%A0%D7%99%D7%9D/?ap={}'\n",
    "\n",
    "    boys_base_page_url = \"https://www.baby-names.co.il/category/%D7%9B%D7%9C-%D7%94%D7%A9%D7%9E%D7%95%D7%AA/page/{}/?ap={}\"\n",
    "\n",
    "    girls_base_url = \"https://www.baby-names.co.il/category/%D7%9B%D7%9C-%D7%94%D7%A9%D7%9E%D7%95%D7%AA/%D7%A9%D7%9E%D7%95%D7%AA-%D7%9C%D7%91%D7%A0%D7%95%D7%AA/?ap={}\"\n",
    "\n",
    "    girls_base_page_url = \"https://www.baby-names.co.il/category/%D7%9B%D7%9C-%D7%94%D7%A9%D7%9E%D7%95%D7%AA/%D7%A9%D7%9E%D7%95%D7%AA-%D7%9C%D7%91%D7%A0%D7%95%D7%AA/page/{}/?ap={}\"\n",
    "\n",
    "    boy_names = []\n",
    "    girl_names = []\n",
    "\n",
    "    for letter in tqdm.tqdm(hebrew_letters):\n",
    "        boy_names = get_names(boys_base_page_url, boys_base_url, letter)\n",
    "        girl_names = get_names(girls_base_page_url, girls_base_url, letter)\n",
    "\n",
    "    return boy_names, girl_names\n",
    "\n",
    "\n",
    "def get_names(base_page_url, base_url, letter):\n",
    "\n",
    "    \"\"\"\n",
    "    Get the names from the page\n",
    "    :param base_page_url: the page url with placeholder for letter and page\n",
    "    :param base_url: the page url with placeholder for letter\n",
    "    :param letter: current letter\n",
    "    :return: list of names\n",
    "    \"\"\"\n",
    "    names = []\n",
    "    name_select_pattern = 'a[title*=פירוש]'\n",
    "\n",
    "    url = base_url.format(letter)\n",
    "    req = requests.get(url)\n",
    "    soup = bs(req.text, 'html.parser')\n",
    "    pages = soup.select('span[class=pagination-meta]')\n",
    "    if len(pages) > 0:\n",
    "        # multiple pages\n",
    "        handle_multiple_pages(base_page_url, letter, names, pages, name_select_pattern)\n",
    "    else:\n",
    "        # single page\n",
    "        for t in soup.select(name_select_pattern):\n",
    "            names.append(t.text)\n",
    "    return names\n",
    "\n",
    "\n",
    "def handle_multiple_pages(base_page_url, letter, names, pages, pattern):\n",
    "    nof_pages = int(pages[0].text.split(\" \")[-1]) + 1\n",
    "    for page in range(nof_pages):\n",
    "        url_with_page = base_page_url.format(page, letter)\n",
    "        req = requests.get(url_with_page)\n",
    "        soup = bs(req.text, 'html.parser')\n",
    "        for t in soup.select(pattern):\n",
    "            names.append(t.text)\n",
    "\n",
    "\n",
    "boys_names3, girls_names3 = create_from_baby_names()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# ** Eyal **\n",
    "\n",
    "# combine the results\n",
    "\n",
    "boys_names_total = list(set(boys_names1 + boys_names2 + boys_names3))\n",
    "girls_names_total = list(set(girls_names1 + girls_names2 + girls_names3))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "(780, 964)"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(boys_names_total), len(girls_names_total)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# ** Eyal **\n",
    "\n",
    "# create the files\n",
    "import os\n",
    "\n",
    "def create_files(output_path, boys_names, girls_names):\n",
    "    \"\"\"\n",
    "    Create the boys and girls text files\n",
    "    If a name appears both in the boys and girls list, it will be written\n",
    "    to a unisex text file\n",
    "    :param output_path: the output path\n",
    "    :param boys_names: list with all the boys names\n",
    "    :param girls_names: list with all the boys names\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "\n",
    "    if not os.path.isdir(output_path):\n",
    "        os.mkdir(output_path)\n",
    "\n",
    "    unisex_names = []\n",
    "\n",
    "    with open(f'./{output_path}/boys.txt', 'w') as fp:\n",
    "        for item in boys_names:\n",
    "            if item in girls_names:\n",
    "                unisex_names.append(item)\n",
    "                girls_names.remove(item)\n",
    "            else:\n",
    "                fp.write(\"%s\\n\" % item)\n",
    "\n",
    "    with open(f'./{output_path}/girls.txt', 'w') as fp:\n",
    "        for item in girls_names:\n",
    "            fp.write(\"%s\\n\" % item)\n",
    "\n",
    "    with open(f'./{output_path}/unisex.txt', 'w') as fp:\n",
    "        for item in unisex_names:\n",
    "            fp.write(\"%s\\n\" % item)\n",
    "\n",
    "data_path = \"./hebrew_names\"\n",
    "create_files(data_path, boys_names_total, girls_names_total)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "././hebrew_names/boys.txt\n",
      "././hebrew_names/unisex.txt\n",
      "././hebrew_names/girls.txt\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "import tqdm\n",
    "\n",
    "\n",
    "def findFiles(path): return glob.glob(path)\n",
    "\n",
    "# print(findFiles('names/*.txt'))\n",
    "\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "\n",
    "# Build the category_lines dictionary, a list of names per language\n",
    "category_lines = {}\n",
    "all_categories = []\n",
    "\n",
    "# Read a file and split into lines\n",
    "def readLines(filename):\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    return [unicodeToAscii(line) for line in lines]\n",
    "\n",
    "for filename in findFiles(f'./{data_path}/*.txt'):\n",
    "    print(filename)\n",
    "    category = os.path.splitext(os.path.basename(filename))[0]\n",
    "    all_categories.append(category)\n",
    "    lines = readLines(filename)\n",
    "    category_lines[category] = lines\n",
    "\n",
    "n_categories = len(all_categories)\n",
    "print(n_categories)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we have ``category_lines``, a dictionary mapping each category\n",
    "(language) to a list of lines (names). We also kept track of\n",
    "``all_categories`` (just a list of languages) and ``n_categories`` for\n",
    "later reference.\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['boys', 'unisex', 'girls']\n"
     ]
    }
   ],
   "source": [
    "print(all_categories)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['גלעד', 'אביגדור', 'אלישב', 'מרדכי', 'ינון']\n"
     ]
    }
   ],
   "source": [
    "# ** Eyal **\n",
    "\n",
    "print(category_lines['boys'][:5])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Turning Names into Tensors\n",
    "--------------------------\n",
    "\n",
    "Now that we have all the names organized, we need to turn them into\n",
    "Tensors to make any use of them.\n",
    "\n",
    "To represent a single letter, we use a \"one-hot vector\" of size\n",
    "``<1 x n_letters>``. A one-hot vector is filled with 0s except for a 1\n",
    "at index of the current letter, e.g. ``\"b\" = <0 1 0 0 0 ...>``.\n",
    "\n",
    "To make a word we join a bunch of those into a 2D matrix\n",
    "``<line_length x 1 x n_letters>``.\n",
    "\n",
    "That extra 1 dimension is because PyTorch assumes everything is in\n",
    "batches - we're just using a batch size of 1 here.\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "torch.Size([4, 1, 32])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# Find letter index from all_letters, e.g. \"a\" = 0\n",
    "def letterToIndex(letter):\n",
    "    return all_letters.find(letter)\n",
    "\n",
    "\n",
    "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
    "def letterToTensor(letter):\n",
    "    tensor = torch.zeros(1, n_letters)\n",
    "    tensor[0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "\n",
    "# Turn a line into a <line_length x 1 x n_letters>,\n",
    "# or an array of one-hot letter vectors\n",
    "def lineToTensor(line):\n",
    "    tensor = torch.zeros(len(line), 1, n_letters)\n",
    "    for i, letter in enumerate(line):\n",
    "        tensor[i][0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "\n",
    "print(letterToTensor('ם'))\n",
    "\n",
    "print(lineToTensor('שלום').size())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Creating the Network\n",
    "====================\n",
    "\n",
    "Before autograd, creating a recurrent neural network in Torch involved\n",
    "cloning the parameters of a layer over several timesteps. The layers\n",
    "held hidden state and gradients which are now entirely handled by the\n",
    "graph itself. This means you can implement a RNN in a very \"pure\" way,\n",
    "as regular feed-forward layers.\n",
    "\n",
    "This RNN module (mostly copied from `the PyTorch for Torch users\n",
    "tutorial <https://pytorch.org/tutorials/beginner/former_torchies/\n",
    "nn_tutorial.html#example-2-recurrent-net>`__)\n",
    "is just 2 linear layers which operate on an input and hidden state, with\n",
    "a LogSoftmax layer after the output.\n",
    "\n",
    ".. figure:: https://i.imgur.com/Z2xbySO.png\n",
    "   :alt:\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        input_combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(input_combined)\n",
    "        output = self.i2o(input_combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "\n",
    "\n",
    "n_hidden = 256\n",
    "rnn = RNN(n_letters, n_hidden, n_categories)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To run a step of this network we need to pass an input (in our case, the\n",
    "Tensor for the current letter) and a previous hidden state (which we\n",
    "initialize as zeros at first). We'll get back the output (probability of\n",
    "each language) and a next hidden state (which we keep for the next\n",
    "step).\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.f = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.o = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.c_tilda = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.Output = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, inp, hid, c):\n",
    "        print (inp.shape)\n",
    "        print(hid.shape)\n",
    "        input_combined = torch.cat((inp, hid), 1)\n",
    "        f = self.sigmoid(self.f(input_combined))\n",
    "        ii = self.sigmoid(self.i(input_combined))\n",
    "        o = self.sigmoid(self.o(input_combined))\n",
    "        c_tilda = self.tanh(self.c_tilda(input_combined))\n",
    "        c ?= torch.mul(f, self.c) + torch.mul(ii, c_tilda)\n",
    "\n",
    "        h = torch.mul(o, self.tanh(c))\n",
    "        out = self.sigmoid(self.Output(h))\n",
    "        return out, h, c\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "\n",
    "lstm = LSTM(n_letters, n_hidden, n_categories)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'c'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [84]\u001B[0m, in \u001B[0;36m<cell line: 4>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m letterToTensor(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mא\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      2\u001B[0m hidden \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mzeros(\u001B[38;5;241m1\u001B[39m, n_hidden)\n\u001B[0;32m----> 4\u001B[0m output, next_hidden \u001B[38;5;241m=\u001B[39m \u001B[43mlstm\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhidden\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      5\u001B[0m output, next_hidden\n",
      "File \u001B[0;32m~/git/courses/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "\u001B[0;31mTypeError\u001B[0m: forward() missing 1 required positional argument: 'c'"
     ]
    }
   ],
   "source": [
    "input = letterToTensor('א')\n",
    "hidden = torch.zeros(1, n_hidden)\n",
    "\n",
    "output, next_hidden = lstm(input, hidden)\n",
    "output, next_hidden"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For the sake of efficiency we don't want to be creating a new Tensor for\n",
    "every step, so we will use ``lineToTensor`` instead of\n",
    "``letterToTensor`` and use slices. This could be further optimized by\n",
    "pre-computing batches of Tensors.\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'c'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [85]\u001B[0m, in \u001B[0;36m<cell line: 6>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m lineToTensor(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mיפית\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      4\u001B[0m hidden \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mzeros(\u001B[38;5;241m1\u001B[39m, n_hidden)\n\u001B[0;32m----> 6\u001B[0m output, next_hidden \u001B[38;5;241m=\u001B[39m \u001B[43mlstm\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhidden\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28mprint\u001B[39m(output)\n",
      "File \u001B[0;32m~/git/courses/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "\u001B[0;31mTypeError\u001B[0m: forward() missing 1 required positional argument: 'c'"
     ]
    }
   ],
   "source": [
    "# ** Eyal **\n",
    "\n",
    "input = lineToTensor('יפית')\n",
    "hidden = torch.zeros(1, n_hidden)\n",
    "\n",
    "output, next_hidden = lstm(input[0], hidden)\n",
    "print(output)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see the output is a ``<1 x n_categories>`` Tensor, where\n",
    "every item is the likelihood of that category (higher is more likely).\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Training\n",
    "========\n",
    "Preparing for Training\n",
    "----------------------\n",
    "\n",
    "Before going into training we should make a few helper functions. The\n",
    "first is to interpret the output of the network, which we know to be a\n",
    "likelihood of each category. We can use ``Tensor.topk`` to get the index\n",
    "of the greatest value:\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('girls', 2)\n"
     ]
    }
   ],
   "source": [
    "def categoryFromOutput(output):\n",
    "    top_n, top_i = output.topk(1)\n",
    "    category_i = top_i[0].item()\n",
    "    return all_categories[category_i], category_i\n",
    "\n",
    "\n",
    "print(categoryFromOutput(output))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will also want a quick way to get a training example (a name and its\n",
    "language):\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category = boys / line = יעקב\n",
      "category = girls / line = תמי\n",
      "category = boys / line = ארי\n",
      "category = girls / line = אביגל\n",
      "category = unisex / line = נואל\n",
      "category = unisex / line = נרי\n",
      "category = unisex / line = תו\n",
      "category = unisex / line = שנהב\n",
      "category = unisex / line = אביטל\n",
      "category = unisex / line = ברית\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def randomChoice(l):\n",
    "    return l[random.randint(0, len(l) - 1)]\n",
    "\n",
    "\n",
    "def randomTrainingExample():\n",
    "    category = randomChoice(all_categories)\n",
    "    line = randomChoice(category_lines[category])\n",
    "    category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)\n",
    "    line_tensor = lineToTensor(line)\n",
    "\n",
    "    return category, line, category_tensor, line_tensor\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    print('category =', category, '/ line =', line)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Training the Network\n",
    "--------------------\n",
    "\n",
    "Now all it takes to train this network is show it a bunch of examples,\n",
    "have it make guesses, and tell it if it's wrong.\n",
    "\n",
    "For the loss function ``nn.NLLLoss`` is appropriate, since the last\n",
    "layer of the RNN is ``nn.LogSoftmax``.\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Each loop of training will:\n",
    "\n",
    "-  Create input and target tensors\n",
    "-  Create a zeroed initial hidden state\n",
    "-  Read each letter in and\n",
    "\n",
    "   -  Keep hidden state for next letter\n",
    "\n",
    "-  Compare final output to target\n",
    "-  Back-propagate\n",
    "-  Return the output and loss\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "learning_rate = 0.005  # If you set this too high, it might explode. If too low, it might not learn\n",
    "\n",
    "\n",
    "def train(category_tensor, line_tensor):\n",
    "    hidden_tensor = lstm.initHidden()\n",
    "\n",
    "    lstm.zero_grad()\n",
    "\n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        print(i)\n",
    "        print(len(line_tensor))\n",
    "        print(hidden_tensor)\n",
    "        output, hidden_tensor = lstm(line_tensor[i], hidden_tensor)\n",
    "\n",
    "    loss = criterion(output, category_tensor)\n",
    "    loss.backward()\n",
    "\n",
    "    # Add parameters' gradients to their values, multiplied by learning rate\n",
    "    for p in lstm.parameters():\n",
    "        p.data.add_(p.grad.data, alpha=-learning_rate)\n",
    "\n",
    "    return output, loss.item()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we just have to run that with a bunch of examples. Since the\n",
    "``train`` function returns both the output and loss we can print its\n",
    "guesses and also keep track of loss for plotting. Since there are 1000s\n",
    "of examples we print only every ``print_every`` examples, and take an\n",
    "average of the loss.\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2])\n",
      "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n",
      "0\n",
      "4\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "torch.Size([1, 32])\n",
      "torch.Size([1, 256])\n",
      "1\n",
      "4\n",
      "tensor([[ 3.0699e-02, -2.3130e-02, -7.9266e-05,  1.2115e-02,  6.5142e-03,\n",
      "         -2.6744e-02, -2.2850e-02,  3.2107e-02, -3.0414e-03,  1.5969e-02,\n",
      "          2.1484e-03,  2.1716e-02,  1.9979e-02, -4.7385e-03,  3.2781e-02,\n",
      "         -1.7943e-02, -6.8728e-03,  1.2531e-02, -1.4732e-02,  1.4891e-02,\n",
      "          3.4577e-02, -1.2481e-02,  2.0498e-03, -3.2627e-02, -1.3000e-02,\n",
      "         -9.4355e-03, -2.4349e-03,  1.8732e-02,  1.4589e-02, -1.1117e-02,\n",
      "         -8.0734e-03,  1.9742e-02, -1.9125e-02,  2.2772e-02, -5.9963e-03,\n",
      "          1.4910e-02, -1.1429e-02,  3.1997e-02, -4.7659e-03, -5.8031e-04,\n",
      "          6.4963e-03,  1.8887e-03, -7.1211e-03, -7.0505e-03, -1.2969e-02,\n",
      "          1.4000e-02,  1.6320e-02,  1.2261e-02,  1.9192e-02,  9.1497e-03,\n",
      "         -1.4467e-02, -1.9343e-02, -1.2877e-02, -6.2954e-03,  2.3764e-02,\n",
      "         -9.4567e-03,  5.3847e-03,  3.1502e-02,  1.6956e-02,  1.4251e-02,\n",
      "         -3.0988e-02, -1.8057e-02, -1.6927e-02,  2.0921e-02, -3.0994e-03,\n",
      "         -1.8624e-02,  1.1876e-02, -1.9562e-02, -1.6197e-02,  3.5104e-03,\n",
      "         -2.2468e-02,  8.9250e-03, -1.7066e-02, -7.5111e-03,  2.6172e-02,\n",
      "         -1.9957e-03, -3.2399e-04,  1.4266e-02,  2.3187e-02,  1.6306e-02,\n",
      "         -2.1576e-02,  2.6102e-02,  1.8423e-02,  1.4994e-03,  1.8482e-02,\n",
      "         -1.6299e-02,  4.4134e-03,  7.0073e-03,  1.7046e-02,  2.3771e-02,\n",
      "          7.5972e-03, -2.2719e-02, -1.3093e-03, -1.1672e-02,  4.3462e-02,\n",
      "         -1.2070e-02, -1.4309e-02,  2.2214e-02,  2.1662e-02, -3.1039e-02,\n",
      "         -9.0014e-03,  3.3561e-03,  1.1038e-02,  3.3642e-02, -1.4758e-02,\n",
      "         -8.9078e-03, -1.1096e-02,  7.7667e-03,  3.3758e-03,  8.5778e-03,\n",
      "         -9.1252e-03,  3.7962e-02, -3.3888e-02,  1.4033e-02, -1.8063e-02,\n",
      "         -3.1989e-03,  2.5874e-02, -1.5802e-02, -9.4713e-03, -4.7169e-03,\n",
      "          1.5540e-02, -2.4376e-02,  2.7243e-02,  1.0161e-02, -4.8033e-03,\n",
      "          8.7606e-03, -3.1978e-03,  3.7340e-03,  9.9241e-04, -1.0930e-02,\n",
      "          1.1022e-02, -1.2810e-02,  1.5738e-02,  7.9468e-03, -2.2984e-02,\n",
      "          1.6421e-02,  1.7285e-03, -1.3422e-02,  1.1510e-02, -6.3823e-03,\n",
      "          1.4716e-02, -5.2242e-04,  2.5152e-02,  1.9147e-02,  5.8772e-04,\n",
      "          4.4455e-03,  3.2436e-02, -2.0318e-02,  1.9543e-02,  2.5404e-02,\n",
      "          1.0464e-02,  8.8006e-03, -2.7094e-02,  2.5965e-02,  1.3706e-02,\n",
      "          1.2268e-02, -2.3700e-02,  3.1703e-02, -2.3294e-03,  1.0329e-02,\n",
      "         -1.5540e-02,  1.2200e-02,  1.0779e-03,  5.3456e-03,  6.1249e-03,\n",
      "          3.2921e-03, -2.2254e-02,  2.1012e-02,  1.4851e-02,  1.2901e-02,\n",
      "          3.2861e-02,  5.4399e-03,  3.5241e-03,  1.3304e-03,  3.0342e-03,\n",
      "          1.8549e-02,  4.8175e-03,  7.7883e-03,  1.5722e-02,  6.9981e-03,\n",
      "         -2.0200e-02, -2.1145e-02, -2.2021e-02, -8.3577e-03,  1.3867e-02,\n",
      "          1.2438e-02,  5.1309e-03,  1.8379e-02,  2.7002e-02, -2.9481e-02,\n",
      "         -6.3892e-03, -4.4402e-02,  2.5320e-03,  1.4471e-02,  1.0001e-02,\n",
      "         -3.2709e-02, -9.6134e-03,  2.0072e-03,  1.3231e-03,  3.4808e-02,\n",
      "         -1.9032e-02,  6.6523e-03,  5.4296e-03,  3.2642e-03,  2.2576e-02,\n",
      "          1.1481e-03, -1.5446e-02,  2.4202e-02, -1.5878e-02, -2.8954e-02,\n",
      "         -1.5156e-04,  1.1552e-03, -2.1118e-02, -1.7160e-02,  1.5723e-02,\n",
      "          1.2902e-02, -4.3365e-03,  3.0996e-03,  3.4814e-02,  2.8004e-02,\n",
      "          2.9418e-02, -1.8257e-02,  2.3114e-02, -1.4402e-03, -7.2130e-03,\n",
      "         -2.6232e-02, -2.4235e-02, -8.2751e-05, -3.4703e-02,  2.2109e-02,\n",
      "         -5.5172e-03, -2.1426e-02,  2.6340e-02, -2.9591e-02,  7.9412e-03,\n",
      "          4.8016e-03,  8.8474e-03, -1.2057e-03, -2.0718e-02, -1.7757e-02,\n",
      "          3.8016e-03,  1.1123e-02,  3.9622e-03, -2.2045e-02, -2.6802e-02,\n",
      "         -3.2448e-03, -2.9574e-02,  1.4407e-02,  4.1852e-03,  2.3209e-02,\n",
      "          9.0407e-03, -2.6396e-02, -1.5449e-02,  3.9960e-02, -1.5138e-02,\n",
      "         -3.7284e-02]], grad_fn=<MulBackward0>)\n",
      "torch.Size([1, 32])\n",
      "torch.Size([1, 256])\n",
      "2\n",
      "4\n",
      "tensor([[ 0.0339, -0.0288,  0.0083, -0.0138, -0.0059, -0.0172, -0.0238,  0.0157,\n",
      "         -0.0040,  0.0076, -0.0047,  0.0231,  0.0062, -0.0282,  0.0137, -0.0081,\n",
      "         -0.0009, -0.0019, -0.0263,  0.0153,  0.0431, -0.0159,  0.0140, -0.0266,\n",
      "         -0.0046, -0.0132, -0.0098,  0.0217,  0.0236, -0.0077, -0.0104,  0.0242,\n",
      "         -0.0110,  0.0173, -0.0044,  0.0268, -0.0035,  0.0217,  0.0052,  0.0105,\n",
      "          0.0030,  0.0025, -0.0147, -0.0025, -0.0153,  0.0231,  0.0232,  0.0157,\n",
      "          0.0102,  0.0167, -0.0056, -0.0069, -0.0144, -0.0087,  0.0325, -0.0124,\n",
      "          0.0086,  0.0142,  0.0246,  0.0250, -0.0113, -0.0223, -0.0146,  0.0080,\n",
      "          0.0150, -0.0236,  0.0131,  0.0003, -0.0136,  0.0073, -0.0284,  0.0240,\n",
      "         -0.0066, -0.0009,  0.0201,  0.0077, -0.0247,  0.0213,  0.0141,  0.0176,\n",
      "         -0.0332,  0.0331,  0.0236,  0.0061,  0.0141, -0.0295,  0.0017,  0.0189,\n",
      "          0.0128,  0.0091, -0.0118, -0.0135,  0.0074, -0.0079,  0.0446, -0.0008,\n",
      "         -0.0143,  0.0266,  0.0304, -0.0137,  0.0040, -0.0037,  0.0108,  0.0177,\n",
      "         -0.0032, -0.0097, -0.0011,  0.0250,  0.0153,  0.0142, -0.0124,  0.0413,\n",
      "         -0.0366,  0.0164, -0.0109, -0.0113,  0.0242, -0.0019, -0.0112, -0.0247,\n",
      "          0.0061, -0.0320,  0.0313, -0.0067, -0.0229, -0.0021, -0.0038, -0.0086,\n",
      "          0.0154, -0.0085, -0.0127, -0.0197,  0.0320,  0.0291, -0.0273,  0.0008,\n",
      "          0.0209, -0.0031,  0.0101, -0.0021,  0.0109,  0.0147,  0.0162,  0.0342,\n",
      "         -0.0144, -0.0094,  0.0318, -0.0250,  0.0071,  0.0367,  0.0090,  0.0250,\n",
      "         -0.0307,  0.0342,  0.0114,  0.0031, -0.0201,  0.0391,  0.0123,  0.0232,\n",
      "         -0.0155, -0.0096,  0.0135,  0.0102,  0.0002,  0.0196, -0.0129,  0.0088,\n",
      "          0.0102,  0.0249,  0.0345, -0.0109,  0.0051, -0.0009,  0.0045,  0.0151,\n",
      "         -0.0142, -0.0114,  0.0121, -0.0006, -0.0300, -0.0298, -0.0228, -0.0089,\n",
      "         -0.0077,  0.0170,  0.0088,  0.0222,  0.0241, -0.0294, -0.0165, -0.0365,\n",
      "          0.0083, -0.0003,  0.0187, -0.0394,  0.0161,  0.0113,  0.0091,  0.0231,\n",
      "          0.0028, -0.0081,  0.0065,  0.0188,  0.0157,  0.0130, -0.0087,  0.0246,\n",
      "         -0.0077, -0.0285, -0.0110,  0.0029, -0.0236, -0.0070,  0.0060,  0.0175,\n",
      "          0.0066,  0.0034,  0.0247,  0.0167,  0.0182, -0.0347,  0.0209, -0.0169,\n",
      "         -0.0129, -0.0202, -0.0112,  0.0087, -0.0208,  0.0161,  0.0108, -0.0059,\n",
      "          0.0109, -0.0266,  0.0165, -0.0025, -0.0010,  0.0053, -0.0229, -0.0044,\n",
      "          0.0072,  0.0058, -0.0028, -0.0199, -0.0352, -0.0040, -0.0277,  0.0069,\n",
      "          0.0060,  0.0334,  0.0131, -0.0178, -0.0052,  0.0439, -0.0144, -0.0321]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "torch.Size([1, 32])\n",
      "torch.Size([1, 256])\n",
      "3\n",
      "4\n",
      "tensor([[ 0.0457, -0.0160,  0.0007, -0.0219,  0.0186,  0.0019, -0.0330,  0.0076,\n",
      "          0.0150,  0.0134, -0.0027,  0.0118,  0.0038, -0.0315,  0.0301, -0.0045,\n",
      "         -0.0059,  0.0053, -0.0079,  0.0218,  0.0441, -0.0149,  0.0007, -0.0245,\n",
      "         -0.0059, -0.0025, -0.0169,  0.0077,  0.0143, -0.0083, -0.0172,  0.0188,\n",
      "         -0.0089,  0.0045, -0.0150,  0.0136, -0.0205,  0.0174, -0.0123,  0.0203,\n",
      "          0.0072, -0.0083, -0.0232,  0.0095, -0.0020,  0.0096,  0.0269,  0.0240,\n",
      "         -0.0070,  0.0095, -0.0093, -0.0107, -0.0068,  0.0003,  0.0194, -0.0072,\n",
      "          0.0246,  0.0124,  0.0120,  0.0198, -0.0170, -0.0184, -0.0056,  0.0173,\n",
      "          0.0155, -0.0380,  0.0122, -0.0066, -0.0187,  0.0024, -0.0266,  0.0134,\n",
      "         -0.0011,  0.0005,  0.0275, -0.0076, -0.0250,  0.0111,  0.0065,  0.0282,\n",
      "         -0.0246,  0.0288,  0.0117, -0.0062,  0.0047, -0.0188,  0.0082,  0.0088,\n",
      "          0.0064,  0.0182, -0.0038, -0.0268,  0.0098,  0.0002,  0.0398, -0.0159,\n",
      "         -0.0137,  0.0356,  0.0355, -0.0345, -0.0020, -0.0178,  0.0033,  0.0043,\n",
      "         -0.0081, -0.0176, -0.0060,  0.0195,  0.0274,  0.0004,  0.0038,  0.0338,\n",
      "         -0.0411,  0.0094, -0.0197, -0.0039,  0.0145,  0.0080, -0.0173, -0.0089,\n",
      "          0.0179, -0.0219,  0.0289,  0.0011, -0.0138,  0.0143,  0.0040, -0.0105,\n",
      "          0.0129, -0.0083, -0.0138, -0.0219,  0.0314,  0.0164, -0.0201,  0.0016,\n",
      "          0.0206, -0.0071,  0.0012,  0.0014,  0.0125,  0.0134,  0.0255,  0.0297,\n",
      "         -0.0300,  0.0100,  0.0402, -0.0358,  0.0080,  0.0297, -0.0053,  0.0280,\n",
      "         -0.0217,  0.0256,  0.0220,  0.0132, -0.0148,  0.0266,  0.0204,  0.0230,\n",
      "         -0.0186, -0.0063,  0.0039,  0.0214,  0.0211,  0.0174, -0.0038,  0.0140,\n",
      "          0.0237,  0.0277,  0.0130, -0.0042,  0.0062,  0.0054, -0.0046,  0.0312,\n",
      "         -0.0148, -0.0078,  0.0114,  0.0121, -0.0227, -0.0275, -0.0248, -0.0110,\n",
      "         -0.0070,  0.0073,  0.0073, -0.0015,  0.0346, -0.0178, -0.0198, -0.0416,\n",
      "         -0.0060,  0.0046,  0.0236, -0.0143,  0.0215,  0.0143, -0.0057,  0.0145,\n",
      "         -0.0081, -0.0032,  0.0053,  0.0268,  0.0323, -0.0015, -0.0130,  0.0198,\n",
      "         -0.0251, -0.0140, -0.0275,  0.0083, -0.0389, -0.0278,  0.0153,  0.0252,\n",
      "          0.0180, -0.0116,  0.0192,  0.0150,  0.0264, -0.0349,  0.0153, -0.0259,\n",
      "         -0.0305, -0.0196, -0.0168,  0.0125, -0.0280,  0.0143,  0.0117, -0.0037,\n",
      "          0.0133, -0.0352, -0.0031,  0.0077,  0.0164, -0.0133, -0.0359, -0.0098,\n",
      "          0.0036,  0.0206, -0.0156, -0.0062, -0.0267,  0.0019, -0.0093,  0.0017,\n",
      "         -0.0040,  0.0388,  0.0074, -0.0199, -0.0127,  0.0271,  0.0053, -0.0257]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "torch.Size([1, 32])\n",
      "torch.Size([1, 256])\n",
      "tensor([2])\n",
      "tensor([[[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n",
      "0\n",
      "4\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "torch.Size([1, 32])\n",
      "torch.Size([1, 256])\n",
      "1\n",
      "4\n",
      "tensor([[ 0.0361, -0.0117,  0.0015, -0.0052,  0.0113, -0.0084, -0.0266,  0.0249,\n",
      "          0.0022,  0.0187, -0.0102,  0.0237,  0.0213, -0.0210,  0.0433, -0.0014,\n",
      "          0.0274,  0.0185, -0.0060,  0.0303,  0.0351, -0.0081, -0.0036, -0.0207,\n",
      "         -0.0196, -0.0193,  0.0050,  0.0115,  0.0147, -0.0066, -0.0229,  0.0206,\n",
      "         -0.0272,  0.0026, -0.0061,  0.0277,  0.0030,  0.0142, -0.0099,  0.0167,\n",
      "          0.0052, -0.0146, -0.0138,  0.0102, -0.0239,  0.0051,  0.0371,  0.0182,\n",
      "          0.0043,  0.0021, -0.0220, -0.0019, -0.0073,  0.0058,  0.0112, -0.0171,\n",
      "          0.0105,  0.0218,  0.0063,  0.0070, -0.0225, -0.0184, -0.0080,  0.0115,\n",
      "         -0.0075, -0.0389,  0.0017, -0.0107, -0.0295,  0.0071, -0.0115,  0.0223,\n",
      "         -0.0257, -0.0023,  0.0106, -0.0081, -0.0377,  0.0041,  0.0146,  0.0227,\n",
      "         -0.0271,  0.0216,  0.0134, -0.0226,  0.0200, -0.0177,  0.0058,  0.0204,\n",
      "          0.0041,  0.0263,  0.0095, -0.0327, -0.0098,  0.0003,  0.0400,  0.0011,\n",
      "         -0.0060,  0.0310,  0.0382, -0.0455, -0.0217, -0.0067,  0.0103,  0.0297,\n",
      "         -0.0023, -0.0064,  0.0134,  0.0135,  0.0015, -0.0009, -0.0065,  0.0361,\n",
      "         -0.0384,  0.0178, -0.0185,  0.0002,  0.0099,  0.0201,  0.0135, -0.0141,\n",
      "          0.0091, -0.0173,  0.0108, -0.0062, -0.0110,  0.0112, -0.0030,  0.0094,\n",
      "          0.0104, -0.0189, -0.0058, -0.0286,  0.0406,  0.0355, -0.0233,  0.0010,\n",
      "          0.0118, -0.0054,  0.0067,  0.0047, -0.0050,  0.0210,  0.0350,  0.0251,\n",
      "         -0.0190,  0.0101,  0.0209, -0.0315,  0.0245,  0.0187,  0.0030,  0.0204,\n",
      "         -0.0341,  0.0329,  0.0115,  0.0101, -0.0319,  0.0238,  0.0097,  0.0212,\n",
      "          0.0070,  0.0036, -0.0087,  0.0267,  0.0252,  0.0132, -0.0099,  0.0296,\n",
      "          0.0161,  0.0119,  0.0172,  0.0015,  0.0112, -0.0022, -0.0142,  0.0269,\n",
      "         -0.0127, -0.0091,  0.0203,  0.0226, -0.0124, -0.0041, -0.0215, -0.0075,\n",
      "          0.0152,  0.0112,  0.0126,  0.0131,  0.0232, -0.0059,  0.0010, -0.0364,\n",
      "         -0.0016,  0.0086,  0.0178, -0.0200,  0.0029, -0.0083,  0.0026,  0.0257,\n",
      "         -0.0069, -0.0201,  0.0183,  0.0166,  0.0333,  0.0112,  0.0092,  0.0107,\n",
      "         -0.0228, -0.0281, -0.0180, -0.0149, -0.0253, -0.0144,  0.0162,  0.0263,\n",
      "          0.0172, -0.0042,  0.0257,  0.0407,  0.0411, -0.0206,  0.0073, -0.0178,\n",
      "         -0.0132, -0.0097, -0.0117,  0.0159, -0.0351,  0.0104,  0.0035, -0.0218,\n",
      "          0.0188, -0.0476,  0.0026,  0.0036,  0.0062, -0.0110, -0.0341, -0.0071,\n",
      "         -0.0013,  0.0055, -0.0082, -0.0033, -0.0140,  0.0112, -0.0225,  0.0049,\n",
      "          0.0050,  0.0409, -0.0066, -0.0179, -0.0203,  0.0368, -0.0058, -0.0217]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "torch.Size([1, 32])\n",
      "torch.Size([1, 256])\n",
      "2\n",
      "4\n",
      "tensor([[ 4.1321e-02, -2.1037e-02, -9.9920e-03, -2.3224e-03,  1.3742e-02,\n",
      "         -1.3188e-02, -3.1977e-02,  1.3300e-02, -5.2244e-03,  2.4205e-03,\n",
      "         -4.7096e-04,  8.9050e-03,  2.8736e-02, -1.0076e-02,  4.5357e-02,\n",
      "          7.1430e-03,  9.8683e-03,  2.9551e-02, -2.1025e-02,  1.8299e-02,\n",
      "          4.1096e-02, -2.9614e-02,  8.2308e-04, -2.4828e-02, -1.2968e-02,\n",
      "         -1.8154e-02, -8.8257e-03,  2.7608e-02,  2.2333e-02, -5.5044e-03,\n",
      "         -1.7698e-02,  1.8824e-02, -1.4005e-02,  1.0796e-02, -2.1743e-02,\n",
      "          1.1091e-02, -1.4886e-02,  2.3055e-02, -1.3763e-02,  1.0138e-03,\n",
      "          9.4642e-04,  2.6149e-03, -1.8824e-02,  1.6050e-03, -1.4032e-02,\n",
      "          1.3386e-02,  1.9774e-02,  2.7612e-02,  1.2201e-02, -5.0068e-03,\n",
      "         -3.4150e-02, -1.0320e-02, -9.5667e-03,  8.0104e-03,  1.5866e-02,\n",
      "         -1.9237e-02,  3.8593e-03,  3.3458e-02, -8.8714e-04,  1.0948e-02,\n",
      "         -2.8252e-02, -1.0337e-02, -1.4726e-02,  5.4926e-03,  4.3479e-03,\n",
      "         -3.4260e-02,  2.0348e-03, -7.5208e-03, -3.6079e-02,  1.0667e-02,\n",
      "         -1.3681e-02,  2.2647e-02, -2.6820e-02, -2.1941e-02,  1.7665e-02,\n",
      "         -1.1708e-02, -1.7764e-02,  1.3793e-02,  2.0219e-02,  9.4081e-03,\n",
      "         -3.1869e-02,  2.2645e-02,  6.8665e-03, -1.0461e-02,  1.7098e-02,\n",
      "         -1.6091e-02,  6.6109e-03,  7.9866e-03,  1.1202e-02,  8.2238e-03,\n",
      "          4.5041e-03, -3.1878e-02, -1.7748e-02, -1.9207e-02,  4.7563e-02,\n",
      "         -1.7805e-03, -7.0670e-03,  3.2860e-02,  3.1682e-02, -3.2994e-02,\n",
      "         -2.1599e-02, -1.4245e-02,  1.1810e-02,  3.0121e-02, -7.8936e-03,\n",
      "         -2.4686e-03, -1.0022e-03,  1.6051e-02, -2.6169e-03, -1.2254e-03,\n",
      "          5.5912e-03,  4.3588e-02, -4.0607e-02,  1.4341e-02, -2.4254e-02,\n",
      "         -3.5667e-03,  1.3664e-02,  3.9680e-03,  1.6685e-03, -1.1129e-02,\n",
      "          6.5977e-03, -2.8638e-02,  2.7120e-02,  5.0819e-03, -1.4890e-02,\n",
      "          1.9995e-02,  1.1997e-02,  9.1326e-03,  2.7180e-03, -9.1766e-03,\n",
      "         -5.5886e-03, -3.1472e-02,  2.5070e-02,  2.3513e-02, -1.0508e-02,\n",
      "          1.5608e-03,  1.1399e-02, -1.2243e-04,  1.2377e-02, -7.1552e-04,\n",
      "          5.5993e-05,  9.6990e-03,  2.4208e-02,  1.5714e-02, -1.2036e-02,\n",
      "         -1.2180e-03,  2.1861e-02, -1.9110e-02,  9.2782e-03,  1.3966e-02,\n",
      "          1.3056e-02,  1.4477e-02, -3.6439e-02,  2.5339e-02,  2.4069e-03,\n",
      "          3.3694e-03, -2.6806e-02,  2.4510e-02,  1.3661e-03,  9.0822e-03,\n",
      "         -4.6676e-03,  7.4629e-03,  1.5184e-02,  2.3891e-02,  1.4177e-02,\n",
      "          8.8800e-03, -8.2197e-03,  2.4018e-02,  9.9696e-03,  9.3210e-04,\n",
      "          2.8678e-02, -8.1833e-03, -3.6931e-03,  5.2139e-03,  2.6393e-03,\n",
      "          2.1574e-02, -5.3713e-03, -2.6524e-03,  2.2100e-02,  1.1093e-02,\n",
      "         -2.4955e-02, -2.0916e-02, -3.3228e-02, -2.8419e-04, -1.3629e-03,\n",
      "          1.4096e-02, -2.6278e-03,  1.0180e-03,  1.6331e-02, -2.1232e-02,\n",
      "          1.2557e-02, -4.5002e-02, -1.1703e-03, -5.9765e-03,  1.2542e-02,\n",
      "         -2.7276e-02,  5.0707e-03, -4.4767e-03, -1.0967e-02,  1.9577e-02,\n",
      "         -1.0867e-02, -1.2431e-02,  8.9596e-03,  7.1512e-03,  4.2009e-02,\n",
      "          1.2830e-02,  4.5779e-03,  1.9517e-02, -1.4535e-02, -2.9718e-02,\n",
      "         -6.6821e-03, -2.1450e-02, -3.4937e-02, -1.8822e-02,  2.3549e-02,\n",
      "          2.1281e-02,  2.7440e-03, -4.1069e-03,  4.2140e-02,  2.4438e-02,\n",
      "          2.5493e-02, -3.7206e-02,  1.7837e-02, -1.8718e-02, -7.6809e-03,\n",
      "         -1.8150e-02, -8.8540e-03, -9.8870e-04, -4.7104e-02,  3.0353e-02,\n",
      "         -5.9770e-04, -6.2514e-03,  2.2490e-02, -4.4427e-02,  1.4128e-02,\n",
      "          1.5494e-02, -5.6343e-03, -9.6906e-03, -2.4160e-02, -1.1612e-02,\n",
      "          1.7381e-03,  1.4547e-02, -3.3343e-03, -2.6850e-02, -2.1387e-02,\n",
      "          5.0019e-03, -7.6909e-03,  4.0470e-03,  1.2044e-02,  3.0585e-02,\n",
      "          5.6192e-03, -1.3938e-02, -4.2583e-03,  3.0242e-02, -8.4577e-03,\n",
      "         -4.1134e-02]], grad_fn=<MulBackward0>)\n",
      "torch.Size([1, 32])\n",
      "torch.Size([1, 256])\n",
      "3\n",
      "4\n",
      "tensor([[ 4.2911e-02, -1.8150e-02, -1.8692e-02, -2.9596e-03,  1.8425e-03,\n",
      "         -1.9036e-02, -1.1329e-02,  1.3915e-02,  1.8106e-02, -1.6321e-03,\n",
      "         -1.3823e-02,  6.7720e-03,  1.0665e-02, -1.3949e-02,  3.7154e-02,\n",
      "          5.3453e-03,  1.0073e-03,  2.1193e-02, -1.3015e-02,  1.2267e-02,\n",
      "          2.5628e-02, -3.2844e-02, -1.2719e-03, -1.7136e-02, -1.6696e-02,\n",
      "         -1.7924e-02, -7.2167e-03,  2.7874e-02,  2.6349e-02, -4.4106e-03,\n",
      "         -5.2661e-03,  3.1034e-02, -2.3289e-02,  4.6740e-03, -2.2144e-02,\n",
      "          2.5034e-02, -1.9647e-02,  1.4335e-02, -9.0252e-03,  1.5772e-02,\n",
      "          2.8607e-03,  8.0778e-03, -2.8918e-02, -1.2109e-02, -5.9642e-03,\n",
      "          1.9868e-02,  3.4111e-02,  1.7259e-02,  8.7469e-03, -9.8139e-03,\n",
      "         -3.1655e-02, -4.5241e-03,  4.2321e-03, -3.2394e-03,  1.4086e-02,\n",
      "         -1.6201e-02,  2.1699e-02,  2.4778e-02,  1.4869e-02,  1.2220e-02,\n",
      "         -2.1781e-02, -1.1381e-03,  2.0747e-03,  2.0952e-02, -2.6018e-03,\n",
      "         -3.6405e-02, -1.2483e-02, -4.1476e-03, -3.3479e-02,  1.4468e-02,\n",
      "         -1.4683e-02,  2.7481e-02, -3.3071e-02, -1.4985e-02,  3.1853e-02,\n",
      "         -1.0519e-03, -2.8404e-02,  1.3300e-02,  4.3170e-03,  7.6272e-03,\n",
      "         -3.0560e-02,  6.0192e-03, -2.2974e-03, -9.7107e-03,  2.7161e-02,\n",
      "         -2.1879e-02,  1.2594e-03,  1.4163e-02,  9.5786e-03,  7.4655e-03,\n",
      "          1.2145e-02, -2.9873e-02, -1.0402e-02, -1.0333e-02,  4.7257e-02,\n",
      "          1.2211e-02, -1.2429e-02,  3.6488e-02,  1.4605e-02, -2.4636e-02,\n",
      "         -2.2346e-02, -2.2063e-02,  2.8084e-03,  3.5239e-02,  6.2137e-03,\n",
      "          4.0216e-04, -9.2045e-04,  3.1365e-02, -4.7394e-03,  7.3481e-03,\n",
      "         -9.2261e-03,  3.0739e-02, -2.5251e-02,  2.0967e-02, -2.8764e-02,\n",
      "         -3.4553e-03,  1.1288e-02, -4.2693e-03,  1.3000e-02, -1.2019e-02,\n",
      "         -3.2013e-03, -2.5477e-02,  1.4518e-02, -6.2896e-03, -3.2035e-02,\n",
      "          1.9841e-02,  8.8688e-03,  1.1871e-02,  2.9487e-03, -2.4408e-03,\n",
      "         -1.5283e-02, -2.8443e-02,  3.7892e-02,  1.5538e-02, -1.3706e-02,\n",
      "          8.2947e-03,  1.7101e-02,  6.5640e-03,  1.7081e-02, -1.4972e-02,\n",
      "          1.6381e-04,  2.2892e-02,  3.9052e-02,  1.4717e-02, -2.1956e-02,\n",
      "         -6.6580e-03,  3.4163e-02, -1.3062e-02,  2.0837e-03,  4.5895e-03,\n",
      "          2.0077e-02,  2.3075e-02, -3.7074e-02,  1.2974e-02,  1.4974e-02,\n",
      "          1.3672e-02, -1.9764e-02,  1.4971e-02,  1.5117e-02,  1.5759e-02,\n",
      "         -1.3498e-02,  3.0698e-03,  1.9877e-02,  2.6716e-02,  7.9254e-03,\n",
      "          1.9263e-02, -1.9696e-03,  2.8231e-02,  8.2268e-03,  2.1301e-02,\n",
      "          2.3823e-02, -4.6716e-03, -2.1889e-03,  1.3277e-02, -4.8654e-04,\n",
      "          3.1221e-02, -1.4812e-02, -7.9420e-03,  2.6167e-02, -3.2151e-04,\n",
      "         -4.0226e-02, -1.2354e-02, -2.1903e-02, -1.5308e-02, -5.3288e-04,\n",
      "          2.1397e-03,  1.1835e-02,  7.0431e-03,  1.3723e-02, -1.3743e-02,\n",
      "          3.2234e-03, -4.0332e-02, -1.2623e-02,  1.3709e-02,  2.6596e-02,\n",
      "         -2.2590e-02,  1.6818e-02, -4.2205e-03, -1.8892e-02,  9.4238e-03,\n",
      "         -9.5383e-03, -5.3051e-03,  1.3994e-02,  1.3254e-02,  4.0638e-02,\n",
      "         -9.5842e-03,  1.6466e-02,  7.5582e-03, -1.2748e-02, -1.8374e-02,\n",
      "         -5.4501e-03, -1.2635e-02, -4.5068e-02, -1.4305e-02,  1.0340e-02,\n",
      "          2.3978e-02, -7.6613e-04, -1.5330e-02,  2.9655e-02,  1.7577e-02,\n",
      "          2.8449e-02, -4.2328e-02,  2.2481e-02, -1.2877e-02, -1.8498e-02,\n",
      "         -2.5122e-02, -2.2121e-02, -1.1859e-03, -3.6146e-02,  3.8779e-02,\n",
      "         -1.2177e-02, -8.5440e-03,  3.0706e-02, -3.8159e-02,  1.5196e-02,\n",
      "          2.6438e-02, -5.9504e-03, -1.9933e-02, -3.4214e-02, -1.1729e-02,\n",
      "          7.2337e-05,  8.6612e-03, -7.3833e-03, -1.7970e-02, -9.2318e-03,\n",
      "          1.4934e-02, -2.0487e-02,  1.3870e-03,  1.9062e-02,  1.7293e-02,\n",
      "          1.3692e-02, -2.6540e-02, -1.5781e-02,  1.8030e-02, -1.3686e-02,\n",
      "         -4.3698e-02]], grad_fn=<MulBackward0>)\n",
      "torch.Size([1, 32])\n",
      "torch.Size([1, 256])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Input \u001B[0;32mIn [82]\u001B[0m, in \u001B[0;36m<cell line: 23>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28mprint\u001B[39m(category_tensor)\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28mprint\u001B[39m(line_tensor)\n\u001B[0;32m---> 27\u001B[0m output, loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcategory_tensor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mline_tensor\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     28\u001B[0m current_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\n\u001B[1;32m     30\u001B[0m \u001B[38;5;66;03m# Print iter number, loss, name and guess\u001B[39;00m\n",
      "Input \u001B[0;32mIn [81]\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(category_tensor, line_tensor)\u001B[0m\n\u001B[1;32m     13\u001B[0m     output, hidden_tensor \u001B[38;5;241m=\u001B[39m lstm(line_tensor[i], hidden_tensor)\n\u001B[1;32m     15\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(output, category_tensor)\n\u001B[0;32m---> 16\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;66;03m# Add parameters' gradients to their values, multiplied by learning rate\u001B[39;00m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m lstm\u001B[38;5;241m.\u001B[39mparameters():\n",
      "File \u001B[0;32m~/git/courses/venv/lib/python3.8/site-packages/torch/_tensor.py:396\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    387\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    388\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    389\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    390\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    394\u001B[0m         create_graph\u001B[38;5;241m=\u001B[39mcreate_graph,\n\u001B[1;32m    395\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs)\n\u001B[0;32m--> 396\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/git/courses/venv/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    168\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    170\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[1;32m    171\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    172\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 173\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    174\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    175\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "n_iters = 100000\n",
    "print_every = 5000\n",
    "plot_every = 1000\n",
    "\n",
    "# Keep track of losses for plotting\n",
    "current_loss = 0\n",
    "all_losses = []\n",
    "\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for iter in range(1, n_iters + 1):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    print(category_tensor)\n",
    "    print(line_tensor)\n",
    "    output, loss = train(category_tensor, line_tensor)\n",
    "    current_loss += loss\n",
    "\n",
    "    # Print iter number, loss, name and guess\n",
    "    if iter % print_every == 0:\n",
    "        guess, guess_i = categoryFromOutput(output)\n",
    "        correct = '✓' if guess == category else '✗ (%s)' % category\n",
    "        print(\n",
    "            '%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), loss, line, guess, correct))\n",
    "\n",
    "    # Add current loss avg to list of losses\n",
    "    if iter % plot_every == 0:\n",
    "        all_losses.append(current_loss / plot_every)\n",
    "        current_loss = 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plotting the Results\n",
    "--------------------\n",
    "\n",
    "Plotting the historical loss from ``all_losses`` shows the network\n",
    "learning:\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "[<matplotlib.lines.Line2D at 0x7f2fb15d1dc0>]"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Evaluating the Results\n",
    "======================\n",
    "\n",
    "To see how well the network performs on different categories, we will\n",
    "create a confusion matrix, indicating for every actual language (rows)\n",
    "which language the network guesses (columns). To calculate the confusion\n",
    "matrix a bunch of samples are run through the network with\n",
    "``evaluate()``, which is the same as ``train()`` minus the backprop.\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41280/830367755.py:35: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_xticklabels([''] + all_categories, rotation=90)\n",
      "/tmp/ipykernel_41280/830367755.py:36: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_yticklabels([''] + all_categories)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Keep track of correct guesses in a confusion matrix\n",
    "confusion = torch.zeros(n_categories, n_categories)\n",
    "n_confusion = 10000\n",
    "\n",
    "\n",
    "# Just return an output given a line\n",
    "def evaluate(line_tensor):\n",
    "    hidden = rnn.initHidden()\n",
    "\n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output, hidden = rnn(line_tensor[i], hidden)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "# Go through a bunch of examples and record which are correctly guessed\n",
    "for i in range(n_confusion):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    output = evaluate(line_tensor)\n",
    "    guess, guess_i = categoryFromOutput(output)\n",
    "    category_i = all_categories.index(category)\n",
    "    confusion[category_i][guess_i] += 1\n",
    "\n",
    "# Normalize by dividing every row by its sum\n",
    "for i in range(n_categories):\n",
    "    confusion[i] = confusion[i] / confusion[i].sum()\n",
    "\n",
    "# Set up plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(confusion.numpy())\n",
    "fig.colorbar(cax)\n",
    "\n",
    "# Set up axes\n",
    "ax.set_xticklabels([''] + all_categories, rotation=90)\n",
    "ax.set_yticklabels([''] + all_categories)\n",
    "\n",
    "# Force label at every tick\n",
    "ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "# sphinx_gallery_thumbnail_number = 2\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can pick out bright spots off the main axis that show which\n",
    "languages it guesses incorrectly, e.g. Chinese for Korean, and Spanish\n",
    "for Italian. It seems to do very well with Greek, and very poorly with\n",
    "English (perhaps because of overlap with other languages).\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.6066, 0.2743, 0.1191],\n        [0.2129, 0.6589, 0.1282],\n        [0.0824, 0.1499, 0.7677]])"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Running on User Input\n",
    "---------------------\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "> בועז\n",
      "(-0.48) boys\n",
      "(-1.51) unisex\n",
      "(-1.84) girls\n",
      "\n",
      "> אודי\n",
      "(-0.56) boys\n",
      "(-1.41) girls\n",
      "(-1.69) unisex\n",
      "\n",
      "> יפית\n",
      "(-0.11) girls\n",
      "(-2.47) unisex\n",
      "(-3.81) boys\n",
      "\n",
      "> אירה\n",
      "(-0.12) girls\n",
      "(-2.60) unisex\n",
      "(-3.21) boys\n"
     ]
    }
   ],
   "source": [
    "# ** Eyal **\n",
    "\n",
    "def predict(input_line, n_predictions=3):\n",
    "    print('\\n> %s' % input_line)\n",
    "    with torch.no_grad():\n",
    "        output = evaluate(lineToTensor(input_line))\n",
    "\n",
    "        # Get top N categories\n",
    "        topv, topi = output.topk(n_predictions, 1, True)\n",
    "        predictions = []\n",
    "\n",
    "        for i in range(n_predictions):\n",
    "            value = topv[0][i].item()\n",
    "            category_index = topi[0][i].item()\n",
    "            print('(%.2f) %s' % (value, all_categories[category_index]))\n",
    "            predictions.append([value, all_categories[category_index]])\n",
    "\n",
    "\n",
    "predict('בועז')\n",
    "predict('אודי')\n",
    "predict('יפית')\n",
    "predict('אירה')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "poqdhYgaQhFT",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "beaf9995-d595-4604-c70f-7597becb638d",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "> בועז\n",
      "(-0.48) boys\n",
      "(-1.51) unisex\n",
      "(-1.84) girls\n",
      "\n",
      "> אודי\n",
      "(-0.56) boys\n",
      "(-1.41) girls\n",
      "(-1.69) unisex\n",
      "\n",
      "> יפית\n",
      "(-0.11) girls\n",
      "(-2.47) unisex\n",
      "(-3.81) boys\n",
      "\n",
      "> אירה\n",
      "(-0.12) girls\n",
      "(-2.60) unisex\n",
      "(-3.21) boys\n"
     ]
    }
   ],
   "source": [
    "def predict(input_line, n_predictions=3):\n",
    "    print('\\n> %s' % input_line)\n",
    "    with torch.no_grad():\n",
    "        output = evaluate(lineToTensor(input_line))\n",
    "\n",
    "        # Get top N categories\n",
    "        topv, topi = output.topk(n_predictions, 1, True)\n",
    "        predictions = []\n",
    "\n",
    "        for i in range(n_predictions):\n",
    "            value = topv[0][i].item()\n",
    "            category_index = topi[0][i].item()\n",
    "            print('(%.2f) %s' % (value, all_categories[category_index]))\n",
    "            predictions.append([value, all_categories[category_index]])\n",
    "\n",
    "\n",
    "predict('בועז')\n",
    "predict('אודי')\n",
    "predict('יפית')\n",
    "predict('אירה')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hCmpZArdQhFT",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The final versions of the scripts `in the Practical PyTorch\n",
    "repo <https://github.com/spro/practical-pytorch/tree/master/char-rnn-classification>`__\n",
    "split the above code into a few files:\n",
    "\n",
    "-  ``data.py`` (loads files)\n",
    "-  ``model.py`` (defines the RNN)\n",
    "-  ``train.py`` (runs training)\n",
    "-  ``predict.py`` (runs ``predict()`` with command line arguments)\n",
    "-  ``server.py`` (serve prediction as a JSON API with bottle.py)\n",
    "\n",
    "Run ``train.py`` to train and save the network.\n",
    "\n",
    "Run ``predict.py`` with a name to view predictions:\n",
    "\n",
    "::\n",
    "\n",
    "    $ python predict.py Hazaki\n",
    "    (-0.42) Japanese\n",
    "    (-1.39) Polish\n",
    "    (-3.51) Czech\n",
    "\n",
    "Run ``server.py`` and visit http://localhost:5533/Yourname to get JSON\n",
    "output of predictions.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EfR76xhUQhFT",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Exercises\n",
    "=========\n",
    "\n",
    "-  Try with a different dataset of line -> category, for example:\n",
    "\n",
    "   -  Any word -> language\n",
    "   -  First name -> gender\n",
    "   -  Character name -> writer\n",
    "   -  Page title -> blog or subreddit\n",
    "\n",
    "-  Get better results with a bigger and/or better shaped network\n",
    "\n",
    "   -  Add more linear layers\n",
    "   -  Try the ``nn.LSTM`` and ``nn.GRU`` layers\n",
    "   -  Combine multiple of these RNNs as a higher level network\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "colab": {
   "name": "char_rnn_classification_tutorial.ipynb.txt",
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}